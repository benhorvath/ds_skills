---
title: "Estimating Value of Data Science Skills by Internet Scraping"
author: "Joel G. & Ben H."
date: "October 21, 2018"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction


## Purpose:

"Which are the most valued data science skills?""



## Analytic Strategy

1. Scrape job listings for data scientists from a jobs posting web site

2. Prepare and clean data for analysis

3. Develop a list of data science hard and soft skills

4. Analyze:

    - Find the most commonly listed skills
    
    - Where salary information is available, compute the mean salary for each skill
    
    
    
# Preparation

## Data Collection

* Source: Indeed.com

* 'data scientist' job postings across five cities:
    
    - New York City
    - San Francisco
    - Chicago
    - Washington, D.C.
    - Seattle
    
* Scraped ~500 listings
    - But due to duplication, final $n = 137$


## Scraping: Query Strings

* We chose to scrape Indeed.com, due to their easy and clean query strings

| element      | example                      |
|--------------|------------------------------|
| base URL     | https://www.indeed.com/jobs? |
| search query | q=data+scientist             |
| location     | l=New+York%2C+NY             |
| page         | start=10                     |



## Scraping: Concatenating Query Strings

Putting it altogether:

`https://www.indeed.com/jobs?q=data+scientist&l=New+York%2C+NY&start=10`

Returns the second page of job listings for `data scientist` located in New York



## Scraping Algorithm

1. For each `city` in the list of `cities`:
    1. Get some page of results, ` pages = seq(0, 100, 10) = (0, 10, 20, ..., 100)`.
    2. For each `page` in `pages`:
        1. Assemble a URL of job results
        2. Read the HTML of the resulting page: `read_html(results_url)`
        3. Extract the approximately 15 links for job postings contained in the HTML: `extract_listing_urls(results_page_html)`
        4. Use `sapply` to `extract_listing_data()` from each of these links
        5. Convert to data frame, save it as TSV
        
        
        
## Scraping Algorithm: Results

50 TSVs saved in the `./data/raw/` directory
        
        






## Data Cleaning

[JOEL]




## Data Cleaning: Salary Information

* Salaray data from Indeed comes in various formats
* `extract_salary()` uses `stringr` and Regular expressions to extract this information, depending on if its a range of salaries or a single salary

```
if (contains_range == TRUE) {
    text_range <- unlist(str_extract_all(s, '\\$[0-9,]+'))
    numeric_range <- as.numeric(str_remove_all(text_range, ',|\\$'))
    return(mean(c(numeric_range[1], numeric_range[2])))
} else {
    as_text <- str_extract_all(s, '\\$[0-9,]+')
    as_numeric <- as.numeric(str_remove_all(as_text, ',|\\$'))
    return(as_numeric)
}
```




## Data Storage

[Joel]




# Analysis

[Two analyses]




## Analysis: By Count

[Joel]



## Analysis: By Salary

[Ben]



# Conclusion

## Further Work

[larger sample size]


## Challenges

[git, scraping duplication, windows v. mac]


## Lessons



## Team Responsibilities

* _Joel_: 

* _Ben_: 


