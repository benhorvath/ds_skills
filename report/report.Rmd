---
title: "DATA 607---Data Science Job Skills"
author: "Joel & Ben"
date: "October 14, 2018"
output:
  html_document:
    toc: true
---
  
```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries:

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(RSQLite)
library(stringr)
library(tidyr)
```

# Introduction

The purpose of this document is to answer the question, "Which are the most valued data science skills?" We collected a sample of data scientist job postings from Indeed.com via web scraping.

After data collection and cleaning, we employed two strategies to value each skill: using salary information from the job listings, and using raw counts. Postings that included salary information were a minority of postings.



# Data Collection

We scraped about 500 data scientist listings from Indeed.com, spread between five large American cities: New York City, San Francisco, Chicago, Washington, D.C., and Seattle.

Indeed.com was chosen because of its simple and clear query string method. The URL of a page of listings consisted of a base URL, a variable for city, and a variable for page number. E.g., the second page of listings for New York City data scientists was `https://www.indeed.com/jobs?q=data+scientist&l=New+York%2C+NY&start=10`. It was thus easy to create a list of ten pages for each city, and then scrape the URLs of the individual job postings on each results page.

We now had a list of individual job postings. Although they were not as structured as we would like, we were able to put some structure to it. Our final data frame had columns: url, job title, company name, Indeed users' score of the comapny, job description, how long the posting had been up, metadata (mostly salary information), the date of scraping, city, and page number of results. 

The actual scrape algorithm is pretty simple:

1. For each `city` in the list of `cities`:
    1. Get some page of results, ` seq(0, 100, 10) = (0, 10, 20, ..., 100)`.
    2. For each `page` in `pages`:
        1. Assemble a URL of job results, e.g., ` https://www.indeed.com/jobs?q=data+scientist&l=Seattle%2C+WA&start=100`
        2. Read the HTML of the resulting page: `read_html(results_url)`
        3. Extract the approximately 15 links for job postings contained in the HTML: `extract_listing_urls(results_page_html)`
        4. Use `sapply` to `extract_listing_data()` from each of these links
        5. Transforming these results into a data.frame, save it to a TSV file
        
The functions `extract_listing_urls()` and `extract_listing_data()` do the bulk of the work. They are present in `scrape.R`. To highlight one interesting technique, `extract_listing_data()` is actually two function. `extract_listing_data_()`---note the underscore at the end---is the main scrape function, which is wrapped in `extract_listing_data()`, which uses R's `tryCatch()` function to handle HTML errors elegantly (404s, etc.).
        
One challenge in the cleaning process was how to store the long string of text in the description column. We employed a nuber of techniques to make this easier, however, we probably should've store data in JSON format. First, we used tab delimited format, second we replaced all whitespace characters in the description with simple spaces, and third, we used `stringR`'s `str_squish()` function.



# Data Cleaning




# Data Storage

(SQL etc.)




# Analysis

Load the clean data from the SQLite table:

```{r}
driver <- SQLite()
con <- dbConnect(driver, dbname='../data/clean/indeed.db')
df <- dbGetQuery( con,'select * from indeed_clean;' )
dbDisconnect(con)
colnames(df) <- as.character(df[1,])
df <- df[-1,]
df$`NA` <- NULL
```


## Analysis: By Count

[Joel section]



## Analysis: By Salary

Another way we might evaluate the relative worth of data science skills is by comparing the salaries of these job offers. Unfortunately, most of these listings do not provide salaries, so we'll have to use a sample much smaller than the original.

First, we'll need to standardize the `metadata` column that seems to exist only for conveying salaries. To begin, we'll subset the dataset to only contain data with salary information. 

Most of the observations are blank, represented by `character(0)` or an empty string `''`. Those empty observations will be filtered out, as well as metadata that does not refer to salary (operationalized by _contains the dollar sign $_) as well as those offering only an hourly wage:

```{r}
del <- df$metadata[1]
salary <- df %>% filter(metadata != del, 
                        metadata != '',
                        str_detect(metadata, '\\$') == TRUE,
                        str_detect(metadata, 'hour') == FALSE)
```

This leaves a total of `r nrow(salary)` observations with salary information. It is presented in a variety of formats that will need to be cleaned:

```{r}
head(salary$metadata)
```

We'll use a function to extract the important data and discard the rest. Note that this function will convert ranges into their averages, e.g., '$100,000 - $110,000' will be converted to '$105,000'.

First, determine if the input string is a valid input. The function is designed to handle common textual representations of salaries, which most often include a dollar sign. If the string does not include a dollar sign, the function assumes the input is invalid and returns `NA`. Otherwise, the function determines if the string contains a salary range or a single salary, via the presence of ` - `. From there, it strips out unnecessary characters, and returns a numeric data type for salary.

```{r}

extract_salary <- function(s) {
    has_dollar_sign <- str_detect(s, '\\$')
    if (has_dollar_sign == FALSE) {
        return(NA)
    } else {
        
        contains_range <- str_detect(s, ' - ')
        
        if (contains_range == TRUE) {
            text_range <- unlist(str_extract_all(s, '\\$[0-9,]+'))
            numeric_range <- as.numeric(str_remove_all(text_range, ',|\\$'))
            return(mean(c(numeric_range[1], numeric_range[2])))
        } else {
            as_text <- str_extract_all(s, '\\$[0-9,]+')
            as_numeric <- as.numeric(str_remove_all(as_text, ',|\\$'))
            return(as_numeric)
        }
    }
}
```

Run a few tests:

```{r}
extract_salary('This is not a salary')
extract_salary('$150,000 (depending on experience)')
extract_salary('$100,000 - $135,000')
```

Apply to our data frame with sapply:

```{r}
salary$salary <- as.numeric(lapply(salary$metadata, extract_salary))
head(salary$salary)
```

### Exploratory Analysis

With our salary data now available, we can begin examining the distribution of salary and its variation across various dimensions.

```{r}
ggplot(salary, aes(salary)) + 
    geom_histogram(binwidth=10000)
```

The salary distribution has a clear central tendancy, though the distribution is pretty wide. We believe the data suggests that a larger sample size would raise several observations further to the right of the above distribution.

The 50th percentile data scientist salary is around $115,000. The minimum paying job is a Data Scientist position for Biz2Credit Inc. in New York City, while the max paying job is for a Senior Data Scientist position for Octane Lending, also in New York City.

```{r}
quantile(salary$salary, c(0, .1, .25, .5, .75, .9, .95, .99))
```



### Skills by Salary

With clean salary and skills data, we are now equipped to examine the relative value each skill in an exact dollar amount. First, we'll look at soft skills, then hard skills, then we'll compare them in aggregate.

Rearrange skills data to be amenable to analysis:

```{r}
skills_long <- salary %>%
    select(url, salary, city, softskill_communication:hardskill_shell) %>%
    gather(skill_name, skill, softskill_communication:hardskill_shell, factor_key=TRUE) %>%
    filter(skill == TRUE) %>%
    select(url, salary, city, skill_name)
```

#### Soft Skills

Further rearrange for soft-skills:

```{r}
soft_skills <- skills_long %>% 
    filter(str_detect(skill_name, 'softskill') == TRUE) %>%
    mutate(skill_name = str_remove(skill_name, 'softskill_')) %>%
    arrange(desc(salary))
```

Graphically represent this data with a box plot:

```{r}
ggplot(soft_skills, aes(skill_name, salary)) +
    geom_boxplot() +
    theme(axis.text.x=element_text(angle=45, hjust=1))
```

The graph gives a sense of the distribution of salaries corresponding to each skill. Below gives the mean value. We see time management appears to be most highly prized, with a mean salary of $120,500. 

```{r}
soft_skills %>% 
    group_by(skill_name) %>% 
    summarize(avg_salary=mean(salary)) %>%
    arrange(desc(avg_salary))
```

Surprisingly, leadership---a ubiquitous and often obnoxious buzzword of the corporate world---does not appear to be so highly valued for data scientists, with a mean salary of $90,000, and a 'lower distribution' than all the other soft skills.



#### Hard Skills

Duplicating this same analysis for hard skills:

```{r}
hard_skills <- skills_long %>% 
    filter(str_detect(skill_name, 'hardskill') == TRUE) %>%
    mutate(skill_name = str_remove(skill_name, 'hardskill_')) %>%
    arrange(desc(salary))

ggplot(hard_skills, aes(skill_name, salary)) +
    geom_boxplot() +
    theme(axis.text.x=element_text(angle=45, hjust=1))
```

The table gives the mean salary of each hard skill:

```{r}
hard_skills %>% 
    group_by(skill_name) %>% 
    summarize(avg_salary=mean(salary)) %>%
    arrange(desc(avg_salary))
```

Although the larger number of hard skills make the graph harder to read, there are perhaps three identifiable groups of higher-, medium-, and lesser-valued skills.



#### Hard Skills v. Soft Skills

The following section is a small experiment to compare the value of soft and hard skills to employers. We will collect all soft skills and hard skills together, and examine their respective mean salaries.

```{r}
hard_soft <- skills_long %>%
    mutate(skill_type = ifelse(str_detect(skills_long$skill_name, 'softskill_') == TRUE, 'soft', 'hard')) %>%
    arrange(desc(salary))

ggplot(hard_soft, aes(skill_type, salary)) +
    geom_boxplot() +
    theme(axis.text.x=element_text(angle=45, hjust=1))
```

Although the mean salary for hard skills is slightly higher than soft skills, the distributions are effectively the same (at least for this low $n = 22$), both in terms of central tendency and variance.

```{r}
hard_soft %>% 
    group_by(skill_type) %>% 
    summarize(avg_salary=mean(salary)) %>%
    arrange(desc(avg_salary))
```

According to this data, we have to conclude that soft and hard skills are valued about equally by employers.


# Conclusion

[Sum conclusions]


Among the top five most highly valued skills include Tensforflow, SQL, data mining, Hive, and Python. This makes sense to me---Tensorflow and deep learning are probably the greatest trend in data science today. Python and SQL are mainstays in data science, and Hive is probably the easiest way to access big data (in a SQL-like language) available today. We were unsurprised that 'pivot table' is not valued so highly---data scientists are not Excel jockeys. More surprising is that the average salary for R is less than $100,000. Though perhaps not---as Python catches up and (arguably) surpasses R as the langauge of choice for datas science, R would be valued less. Perhaps a flood of new R-trained data science and statistics graduates are depressing its price in the market!

The soft skills were more surprising. Leadership---a ubiquitous and often obnoxious buzzword of the corporate world---does not appear to be so highly valued for data scientists, with a mean salary of $90,000, and a 'lower distribution' than all the other soft skills. Data science projects often lack a discrete beginning and end, so time management was the most prized soft skill. Communication was also highly valued, which is a sensible desire for technical skilled workers who oftne have to provide non-technical management with recommendations and analyses.

Hard skills and soft skills in general appear to be equally prized by employers.

[last paragraph]Caution is warranted, however. These results are based on a small sample size. Further work could validate them by collecting much more job listings with more geographical diversity. Although we included geography in this data set, we did not explore this variation.


